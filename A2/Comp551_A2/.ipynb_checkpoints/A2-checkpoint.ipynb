{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For each question, run sequentially)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For windows system, you may need to change the dataset path to \"hwk2_datasets\\\\\\\\\\[file_name\\]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate train, validation and test dataset based on mean vectors and covariance matrix given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is shuffled and partitioned with 60% train, 20% validation and 20% test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the data of classNegative is labeled to negative and classPositive is labeled to positive. (Adding -1s and 1s to the last column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to read CSV file and returning a np array containing the data\n",
    "def csvReader(filepath):\n",
    "    with open(filepath) as csvFile:\n",
    "        dataSet = csv.reader(csvFile, delimiter=',')\n",
    "        data = []\n",
    "        for row in dataSet:\n",
    "            inputTemp = []\n",
    "            for i in range(len(row)):\n",
    "                if(row[i] != ''):   # somehow the initial csv data has empty string, need this step to filter it\n",
    "                    inputTemp.append(float(row[i]))\n",
    "            data.append(inputTemp)\n",
    "    data = np.array(data, dtype='float')\n",
    "    return data\n",
    "\n",
    "\n",
    "# Question 1 - Generate dataset\n",
    "def generateDataset(covPath, m0Path, m1Path):\n",
    "    cov = csvReader(covPath)\n",
    "    mean0 = csvReader(m0Path)\n",
    "    mean1 = csvReader(m1Path)\n",
    "    c0 = np.random.multivariate_normal(mean0[0], cov, 2000)\n",
    "    classNegative = -1 * np.ones((2000, 21), dtype='float')\n",
    "    classNegative[:,:-1] = c0\n",
    "    c1 = np.random.multivariate_normal(mean1[0], cov, 2000)\n",
    "    classPositive = np.ones((2000, 21), dtype='float')\n",
    "    classPositive[:,:-1] = c1\n",
    "\n",
    "    random.shuffle(classNegative)\n",
    "    random.shuffle(classPositive)\n",
    "    testBound = int(len(classNegative) * 0.2)\n",
    "    validBound = int(len(classNegative) * 0.2)\n",
    "\n",
    "    testSet = []\n",
    "    validSet = []\n",
    "    trainSet = []\n",
    "    for i in range(2000):\n",
    "        if (i < testBound):\n",
    "            testSet.append(classNegative[i])\n",
    "            testSet.append(classPositive[i])\n",
    "        elif (i >= testBound and i < testBound + validBound):\n",
    "            validSet.append(classNegative[i])\n",
    "            validSet.append(classPositive[i])\n",
    "        else:\n",
    "            trainSet.append(classNegative[i])\n",
    "            trainSet.append(classPositive[i])\n",
    "\n",
    "    random.shuffle(testSet)\n",
    "    random.shuffle(trainSet)\n",
    "    random.shuffle(validSet)\n",
    "\n",
    "    with open('hwk2_datasets/DS1-test.csv', 'w') as csvfile:\n",
    "        csvWriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        for i in range(len(testSet)):\n",
    "            csvWriter.writerow(testSet[i])\n",
    "\n",
    "    with open('hwk2_datasets/DS1-valid.csv', 'w') as csvfile:\n",
    "        csvWriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        for i in range(len(validSet)):\n",
    "            csvWriter.writerow(validSet[i])\n",
    "\n",
    "    with open('hwk2_datasets/DS1-train.csv', 'w') as csvfile:\n",
    "        csvWriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        for i in range(len(trainSet)):\n",
    "            csvWriter.writerow(trainSet[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset generated are 'DS1-test.csv', 'DS1-valid.csv' and 'DS1-train.csv'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, calculate the parameters w0 and w1 based on equations on slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Before calculating  w0 and w1, we need to calculate P1, P2, mean1, mean2 and covariance matrix.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gda(filePath):\n",
    "    trainData = csvReader(filePath)\n",
    "    sum1 = np.zeros(trainData[0].shape, dtype='float')\n",
    "    sum2 = np.zeros(trainData[0].shape, dtype='float')\n",
    "    count1 = 0;\n",
    "    count2 = 0;\n",
    "    for i in range(len(trainData)):\n",
    "        if (trainData[i][len(trainData[0])-1] == -1):\n",
    "            sum1 = np.add(sum1,trainData[i])\n",
    "            count1 += 1\n",
    "        elif (trainData[i][len(trainData[0])-1] == 1):\n",
    "            sum2 = np.add(sum2, trainData[i])\n",
    "            count2 += 1\n",
    "    m1 = np.divide(sum1, count1)\n",
    "    m2 = np.divide(sum2, count2)\n",
    "    m1 = m1[:-1].reshape(1,-1) # remove labeled 1s and -1s at the last column\n",
    "    m2 = m2[:-1].reshape(1, -1)\n",
    "    P1 = count1/len(trainData)\n",
    "    P2 = count2/len(trainData)\n",
    "    sum = np.zeros((len(m1[0]), len(m2[0])))\n",
    "    for i in range(len(trainData)):\n",
    "        part1 = np.subtract(trainData[i][:-1], m1)\n",
    "        part2 = np.subtract(trainData[i][:-1], m2)\n",
    "        S1 = np.dot(np.transpose(part1), part1)\n",
    "        S2 = np.dot(np.transpose(part2), part2)\n",
    "        sum = np.add(np.add(S1, S2), sum)\n",
    "\n",
    "    cov = np.divide(sum, count1+count2)\n",
    "    cov_inv = np.linalg.inv(cov)\n",
    "    term1 = 1/2 * np.dot(np.dot(m1, cov_inv), np.transpose(m1))\n",
    "    term2 = 1/2 * np.dot(np.dot(m2, cov_inv), np.transpose(m2))\n",
    "    w0 = np.subtract(term2, term1) + np.log(P1) - np.log(P2)\n",
    "    w1 = np.dot(cov_inv, np.transpose(np.subtract(m1, m2)))\n",
    "    w = [w0, w1]\n",
    "    print('w0 = ' + str(w0[0][0]))\n",
    "    print('w1 = ' + str(w1))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we validate the performance of our model by applying test set that we generated before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0 = 1.9463125787853652\n",
      "w1 = [[ 1.02193507]\n",
      " [-0.61584269]\n",
      " [-0.44156509]\n",
      " [-0.27127862]\n",
      " [-0.74051604]\n",
      " [-0.29672473]\n",
      " [ 1.30321243]\n",
      " [-1.71359568]\n",
      " [-2.13595237]\n",
      " [ 0.62525576]\n",
      " [-0.91837962]\n",
      " [-0.91067288]\n",
      " [ 1.1556343 ]\n",
      " [ 0.99645741]\n",
      " [-0.38852566]\n",
      " [ 0.91743229]\n",
      " [ 2.18270508]\n",
      " [-0.48336646]\n",
      " [-0.1325924 ]\n",
      " [-0.33832297]]\n",
      "Accuracy = 0.95875\n",
      "Precision = 0.9508599508599509\n",
      "Recall = 0.9675\n",
      "F1 measure = 0.9591078066914498\n"
     ]
    }
   ],
   "source": [
    "def validateGDA(filepath, w):\n",
    "    data = csvReader(filepath)\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    w0 = w[0]\n",
    "    w1 = w[1]\n",
    "    for i in range(len(data)):\n",
    "        a = w0[0][0] + np.dot(data[i][:-1], w1)\n",
    "        if (a > 0):\n",
    "            if(data[i][len(data[0]) - 1] == -1):\n",
    "                TN += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        elif (a < 0):\n",
    "            if (data[i][len(data[0]) - 1] == 1):\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    accuracy = (TP + TN) / (TP + TN + FN + FP)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1measure = (2*precision*recall) / (precision+recall)\n",
    "    print('Accuracy = ' + str(accuracy))\n",
    "    print('Precision = ' + str(precision))\n",
    "    print('Recall = ' + str(recall))\n",
    "    print('F1 measure = ' + str(F1measure))\n",
    "\n",
    "\n",
    "w = gda('hwk2_datasets/DS1-train.csv')\n",
    "validateGDA('hwk2_datasets/DS1-test.csv', w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying Euclidean Distance equation, we can obtain an array of distance and find the k nearest neighbors. And then assign the input x to relative class (the most common class among neighbors). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(X_train, X_test, k):\n",
    "    d = []\n",
    "    for i in range(len(X_train)):\n",
    "        sum = 0\n",
    "        for j in range(len(X_test) - 1):\n",
    "            sum += np.square(X_train[i][j]-X_test[j])\n",
    "        d.append([np.sqrt(sum), i, X_train[i][len(X_train[0]) - 1]])\n",
    "\n",
    "    d = sorted(d)\n",
    "    nn = []\n",
    "    for i in range (k):\n",
    "        nn.append(d[i][2])\n",
    "    prediction = np.sum(nn)\n",
    "    if (prediction > 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the KNN classifier to classify each data sample in the test set and compare with the actual class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will also find the best K in range 1 to 20 (based on F1 measure), however, since the computation takes really long time, I'll set the range to 5 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: we need to normalize the dataset in KNN classifier, I used sklearn library here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateKNN(trainDataset, testDataset):\n",
    "    trainData = csvReader(trainDataset)\n",
    "    trainData[:, :-1] = preprocessing.normalize(trainData[:, :-1], axis=0)  # need to normalize the data\n",
    "    testData = csvReader(testDataset)\n",
    "    testData[:, :-1] = preprocessing.normalize(testData[:, :-1], axis=0)\n",
    "\n",
    "    bestk = 0\n",
    "    bestF1 = 0\n",
    "\n",
    "    for i in range (1, 6):\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        for j in range(len(testData)):\n",
    "            prediction = KNN(trainData, testData[j], i)\n",
    "            if (prediction > 0):\n",
    "                if (trainData[j][len(trainData[0]) - 1] == -1):\n",
    "                    TN += 1\n",
    "                else:\n",
    "                    FN += 1\n",
    "            else:\n",
    "                if (trainData[j][len(trainData[0]) - 1] == 1):\n",
    "                    TP += 1\n",
    "                else:\n",
    "                    FP += 1\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        F1measure = (2 * precision * recall) / (precision + recall)\n",
    "        print('k = ' + str(i))\n",
    "        print(' F1 Measure = ' + str(F1measure))\n",
    "        if (F1measure > bestF1):\n",
    "            bestF1 = F1measure\n",
    "            bestk = i\n",
    "\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(len(testData)):\n",
    "        prediction = KNN(trainData, testData[i], bestk)\n",
    "        if (prediction > 0):\n",
    "            if (trainData[i][len(trainData[0]) - 1] == -1):\n",
    "                TN += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        else:\n",
    "            if (trainData[i][len(trainData[0]) - 1] == 1):\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    accuracy = (TP + TN) / (TP + TN + FN + FP)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1measure = (2 * precision * recall) / (precision + recall)\n",
    "    print('Best K = ' + str(bestk))\n",
    "    print('Accuracy = ' + str(accuracy))\n",
    "    print('Precision = ' + str(precision))\n",
    "    print('Recall = ' + str(recall))\n",
    "    print('F1 measure = ' + str(F1measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      " F1 Measure = 0.4673913043478261\n",
      "k = 2\n",
      " F1 Measure = 0.5360824742268041\n",
      "k = 3\n",
      " F1 Measure = 0.4775725593667546\n",
      "k = 4\n",
      " F1 Measure = 0.5166666666666667\n",
      "k = 5\n",
      " F1 Measure = 0.478494623655914\n",
      "Best K = 2\n",
      "Accuracy = 0.49375\n",
      "Precision = 0.4746450304259635\n",
      "Recall = 0.6157894736842106\n",
      "F1 measure = 0.5360824742268041\n"
     ]
    }
   ],
   "source": [
    "validateKNN('hwk2_datasets/DS1-train.csv', 'hwk2_datasets/DS1-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When K = 2, it gives the best fit with F1 measure = 0.5360824742268041"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Question 1, generate dataset 2 by a mixture of 3 Gaussians with mixture probability (0.1, 0.42, 0.48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(m11, m12, m13, m21, m22, m23, c1, c2, c3):\n",
    "    mean11 = csvReader(m11)\n",
    "    mean12 = csvReader(m12)\n",
    "    mean13 = csvReader(m13)\n",
    "    mean21 = csvReader(m21)\n",
    "    mean22 = csvReader(m22)\n",
    "    mean23 = csvReader(m23)\n",
    "    cov1 = csvReader(c1)\n",
    "    cov2 = csvReader(c2)\n",
    "    cov3 = csvReader(c3)\n",
    "    class11 = np.random.multivariate_normal(mean11[0], cov1, 2000)\n",
    "    class12 = np.random.multivariate_normal(mean12[0], cov2, 2000)\n",
    "    class13 = np.random.multivariate_normal(mean13[0], cov3, 2000)\n",
    "    class21 = np.random.multivariate_normal(mean21[0], cov1, 2000)\n",
    "    class22 = np.random.multivariate_normal(mean22[0], cov2, 2000)\n",
    "    class23 = np.random.multivariate_normal(mean23[0], cov3, 2000)\n",
    "    class1_temp = []\n",
    "    class2_temp = []\n",
    "\n",
    "    for i in range(2000):\n",
    "        choice = np.random.choice([1,2,3,], 1, p=[0.1,0.42,0.48])\n",
    "        if (choice == 1):\n",
    "            class1_temp.append(class11[i])\n",
    "            class2_temp.append(class21[i])\n",
    "        elif (choice == 2):\n",
    "            class1_temp.append(class12[i])\n",
    "            class2_temp.append(class22[i])\n",
    "        else:\n",
    "            class1_temp.append(class13[i])\n",
    "            class2_temp.append(class23[i])\n",
    "\n",
    "    classNegative = -1 * np.ones((2000, 21), dtype='float')\n",
    "    classNegative[:,:-1] = class1_temp\n",
    "    classPositive = np.ones((2000, 21), dtype='float')\n",
    "    classPositive[:,:-1] = class2_temp\n",
    "\n",
    "    random.shuffle(classNegative)\n",
    "    random.shuffle(classPositive)\n",
    "\n",
    "    testSet = []\n",
    "    validSet = []\n",
    "    trainSet = []\n",
    "    testBound = int(len(classNegative) * 0.2)\n",
    "    validBound = int(len(classNegative) * 0.2)\n",
    "\n",
    "    for i in range(2000):\n",
    "        if (i < testBound):\n",
    "            testSet.append(classNegative[i])\n",
    "            testSet.append(classPositive[i])\n",
    "        elif (i >= testBound and i < testBound + validBound):\n",
    "            validSet.append(classNegative[i])\n",
    "            validSet.append(classPositive[i])\n",
    "        else:\n",
    "            trainSet.append(classNegative[i])\n",
    "            trainSet.append(classPositive[i])\n",
    "\n",
    "    random.shuffle(testSet)\n",
    "    random.shuffle(trainSet)\n",
    "    random.shuffle(validSet)\n",
    "\n",
    "    with open('hwk2_datasets/DS2-test.csv', 'w') as csvfile:\n",
    "        csvWriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        for i in range(len(testSet)):\n",
    "            csvWriter.writerow(testSet[i])\n",
    "\n",
    "    with open('hwk2_datasets/DS2-valid.csv', 'w') as csvfile:\n",
    "        csvWriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        for i in range(len(validSet)):\n",
    "            csvWriter.writerow(validSet[i])\n",
    "\n",
    "    with open('hwk2_datasets/DS2-train.csv', 'w') as csvfile:\n",
    "        csvWriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        for i in range(len(trainSet)):\n",
    "            csvWriter.writerow(trainSet[i])\n",
    "\n",
    "# generateData('hwk2_datasets/DS2_c1_m1.txt', 'hwk2_datasets/DS2_c1_m2.txt', 'hwk2_datasets/DS2_c1_m3.txt',\n",
    "#              'hwk2_datasets/DS2_c2_m1.txt', 'hwk2_datasets/DS2_c2_m2.txt', 'hwk2_datasets/DS2_c2_m3.txt',\n",
    "#              'hwk2_datasets/DS2_Cov1.txt', 'hwk2_datasets/DS2_Cov2.txt', 'hwk2_datasets/DS2_Cov3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat GDA on dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0 = 0.00944858874154364\n",
      "w1 = [[-0.01587187]\n",
      " [-0.03097841]\n",
      " [ 0.02603874]\n",
      " [-0.00485993]\n",
      " [ 0.00947184]\n",
      " [ 0.00059935]\n",
      " [ 0.01926517]\n",
      " [ 0.08241396]\n",
      " [-0.02991102]\n",
      " [ 0.01269773]\n",
      " [-0.01779393]\n",
      " [ 0.00205633]\n",
      " [ 0.00282658]\n",
      " [-0.02220273]\n",
      " [-0.0197494 ]\n",
      " [ 0.02114536]\n",
      " [-0.02914988]\n",
      " [-0.03820653]\n",
      " [ 0.01388067]\n",
      " [ 0.01200783]]\n",
      "Accuracy = 0.535\n",
      "Precision = 0.5333333333333333\n",
      "Recall = 0.56\n",
      "F1 measure = 0.5463414634146342\n"
     ]
    }
   ],
   "source": [
    "w = gda('hwk2_datasets/DS2-train.csv')\n",
    "validateGDA('hwk2_datasets/DS2-test.csv', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything drops significantly on dataset 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat KNN on dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      " F1 Measure = 0.5341317365269461\n",
      "k = 2\n",
      " F1 Measure = 0.5896656534954406\n",
      "k = 3\n",
      " F1 Measure = 0.5193236714975845\n",
      "k = 4\n",
      " F1 Measure = 0.6021052631578947\n",
      "k = 5\n",
      " F1 Measure = 0.5364705882352943\n",
      "Best K = 4\n",
      "Accuracy = 0.5275\n",
      "Precision = 0.5267034990791897\n",
      "Recall = 0.7027027027027027\n",
      "F1 measure = 0.6021052631578947\n"
     ]
    }
   ],
   "source": [
    "validateKNN('hwk2_datasets/DS2-train.csv', 'hwk2_datasets/DS2-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When K = 4, it gives the best fit with F1 measure = 0.6021052631578947 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
